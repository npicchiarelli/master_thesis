\documentclass[../../master_thesis_np.tex]{subfiles}
\graphicspath{{./imgs/}}

\begin{document}
\chapter{Machine Learning Analysis}
\section{Methods}
The objective of this part is to build a machine learning-based tool to infer the interaction potential starting from the history of positions and velocities of an ensemble of Active Brownian Particles.
As explained in section \ref{literature}, one could use as input some global attribute of the system, such as the radial distribution function, but, as showed by \citeauthor{bag_interaction_2021}, this approach does not work well in out-of-equilibrium cases where an active velocity is present.

\subsection{The Graph Neural Network}
Here, we tried to replicate the approach used by \citeauthor{ruiz-garcia_discovering_2024}, where a vector of positions and orientations of a set of ABPs is given as input to a Graph Neural Network which tries to predict resulting velocities.
Our GNN is structured as explained in section \ref{literature}, namely with a node and a message function. 
Both of them have 4 layers, with 300 hidden nodes; each hidden layer has a ReLU (Rectified Linear Unit) activation function, while the output layer of each Network is a simple linear layer without any activation.
As for Figure \ref{fig:ruiz1}, the input layer of message function is $2n_f$ dimensional, where $n_f$ is the number of single particle features, namely $x$, $y$, $\theta$ to have information about positions and orientations of a pair of interacting agents.
Messages are aggregated using sum as an aggregation function (\verb|'add'| in PyG jargon) to respect the physics of the problem.
Node function's input dimension is $n_f + n_m$ where $n_m$ is the output dimension of the message function; this is done because node function has the purpose of taking the aggregated message from all the senders (particle inside a threshold distance $\Gamma$) along with the single features of the receiver and trying to predict receiver's velocity (acceleration in Newtonian dynamics).

Here it is important to note that the history is not relevant: the network just takes one instant at a time and predicts instantaneous velocities starting from positions and orientations, without knowing what happens before or after.
Instantaneous velocities are computed dividing the difference of consecutive positions by the integration time interval, then they get checked for big jumps, that happen when periodic boundary condition correction take place.

%The problem of message dimension $n_m$ is tackled in section \ref{literature}

\subsection{Simulations}
To understand the role of the potential's functional form in prediction results, we tried to train and test the network on two different simulations, one done using a spring potential with $k_s = \SI{e-2}{\newton\per\meter}$ and $x_0 = \SI{4}{\um}$ and the other using a LJ potential with $\sigma = \SI{4}{\um}$ and $\sigma\epsilon = \SI{0.01}{\micro\joule}$.
These potential have the difference in range: without limiting our interaction threshold radius, elastic force not only has effect at long distance but its absolute value increases, while LJ is a short range potential.
We expect the network to perform worse on LJ since it has less relevant interactions to learn from.

\section{Training and Testing}
\subsection{Spring Potential}
\subsection{Lennard Jones Potential}
\section{Conclusions}
\end{document}